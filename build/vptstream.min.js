(function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
"use strict";

module.exports = function (strings) {
  if (typeof strings === 'string') strings = [strings];
  var exprs = [].slice.call(arguments, 1);
  var parts = [];

  for (var i = 0; i < strings.length - 1; i++) {
    parts.push(strings[i], exprs[i] || '');
  }

  parts.push(strings[i]);
  return parts.join('');
};

},{}],2:[function(require,module,exports){
"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
Object.defineProperty(exports, "VPTStream", {
  enumerable: true,
  get: function get() {
    return _vptstream["default"];
  }
});

var _vptstream = _interopRequireDefault(require("./vptstream"));

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { "default": obj }; }

function _typeof(obj) { "@babel/helpers - typeof"; if (typeof Symbol === "function" && typeof Symbol.iterator === "symbol") { _typeof = function _typeof(obj) { return typeof obj; }; } else { _typeof = function _typeof(obj) { return obj && typeof Symbol === "function" && obj.constructor === Symbol && obj !== Symbol.prototype ? "symbol" : typeof obj; }; } return _typeof(obj); }

//Make it global
if (typeof window !== 'undefined' && _typeof(window.THREE) === 'object') {
  window.VPTStream = _vptstream["default"];
} else {
  console.warn('[vpt-stream] It seems like THREE is not included in your code, try including it before vpt-stream.js');
}

},{"./vptstream":3}],3:[function(require,module,exports){
"use strict";

function _typeof(obj) { "@babel/helpers - typeof"; if (typeof Symbol === "function" && typeof Symbol.iterator === "symbol") { _typeof = function _typeof(obj) { return typeof obj; }; } else { _typeof = function _typeof(obj) { return obj && typeof Symbol === "function" && obj.constructor === Symbol && obj !== Symbol.prototype ? "symbol" : typeof obj; }; } return _typeof(obj); }

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports["default"] = void 0;

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError("Cannot call a class as a function"); } }

function _defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ("value" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } }

function _createClass(Constructor, protoProps, staticProps) { if (protoProps) _defineProperties(Constructor.prototype, protoProps); if (staticProps) _defineProperties(Constructor, staticProps); return Constructor; }

function _inherits(subClass, superClass) { if (typeof superClass !== "function" && superClass !== null) { throw new TypeError("Super expression must either be null or a function"); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, writable: true, configurable: true } }); if (superClass) _setPrototypeOf(subClass, superClass); }

function _setPrototypeOf(o, p) { _setPrototypeOf = Object.setPrototypeOf || function _setPrototypeOf(o, p) { o.__proto__ = p; return o; }; return _setPrototypeOf(o, p); }

function _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }

function _possibleConstructorReturn(self, call) { if (call && (_typeof(call) === "object" || typeof call === "function")) { return call; } return _assertThisInitialized(self); }

function _assertThisInitialized(self) { if (self === void 0) { throw new ReferenceError("this hasn't been initialised - super() hasn't been called"); } return self; }

function _isNativeReflectConstruct() { if (typeof Reflect === "undefined" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === "function") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }

function _getPrototypeOf(o) { _getPrototypeOf = Object.setPrototypeOf ? Object.getPrototypeOf : function _getPrototypeOf(o) { return o.__proto__ || Object.getPrototypeOf(o); }; return _getPrototypeOf(o); }

var glsl = require('glslify');

var rgbdFrag = glsl(["#define GLSLIFY 1\nuniform sampler2D map;\nuniform float opacity;\nuniform float width;\nuniform float height;\n\nvarying vec4 ptColor;\nvarying vec2 vUv;\nvarying vec3 debug;\n\nvoid main() {\n\n    if( ptColor.a <= 0.0){\n        discard;\n    }\n\n    vec4 colorSample = ptColor;\n    colorSample.a *= (ptColor.a * opacity);\n\n    gl_FragColor = colorSample;\n}"]);
var rgbdVert = glsl(["#define GLSLIFY 1\nuniform vec2 focalLength;//fx,fy\nuniform vec2 principalPoint;//ppx,ppy\nuniform vec2 imageDimensions;\nuniform mat4 extrinsics;\nuniform float width;\nuniform float height;\nuniform float scale;\nuniform sampler2D map;\n\nuniform float pointSize;\nuniform float depthMin;\nuniform float depthMax;\nuniform vec3 thresholdMin;\nuniform vec3 thresholdMax;\nvarying vec4 ptColor;\nvarying vec2 vUv;\nvarying vec3 debug;\n\nconst float _DepthSaturationThreshhold = 0.3; //a given pixel whose saturation is less than half will be culled (old default was .5)\nconst float _DepthBrightnessThreshold = 0.3; //a given pixel whose brightness is less than half will be culled (old default was .9)\nconst float  _Epsilon = .03;\n\n#define BRIGHTNESS_THRESHOLD_OFFSET 0.01\n#define FLOAT_EPS 0.00001\n#define CLIP_EPSILON 0.005\n\nvec3 rgb2hsv(vec3 c)\n{\n    vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);\n    vec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g));\n    vec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r));\n    float d = q.x - min(q.w, q.y);\n    return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + FLOAT_EPS)), d / (q.x + FLOAT_EPS), q.x);\n}\n\nfloat depthForPoint(vec2 texturePoint)\n{       \n    vec2 centerpix = vec2(1.0/width, 1.0/height) * 0.5;\n    texturePoint += centerpix;\n\n    // clamp to texture bounds - 0.5 pixelsize so we do not sample outside our texture\n    texturePoint = clamp(texturePoint, centerpix, vec2(1.0, 0.5) - centerpix);\n    vec4 depthsample = texture2D(map, texturePoint);\n    vec3 depthsamplehsv = rgb2hsv(depthsample.rgb);\n    return depthsamplehsv.b > _DepthBrightnessThreshold + BRIGHTNESS_THRESHOLD_OFFSET ? depthsamplehsv.r : 0.0;\n}\n\n//https://stackoverflow.com/questions/12751080/glsl-point-inside-box-test/37426532\nfloat insideBox3D(vec3 v, vec3 bottomLeft, vec3 topRight) {\n    vec3 s = step(bottomLeft, v) - step(topRight, v);\n    return s.x * s.y * s.z; \n}\n\nvoid main()\n{   \n    vec4 texSize = vec4(1.0 / width, 1.0 / height, width, height);\n    vec2 basetex = position.xy + vec2(0.5,0.5);\n\n    // we align our depth pixel centers with the center of each quad, so we do not require a half pixel offset\n    vec2 depthTexCoord = basetex * vec2(1.0, 0.5);\n    vec2 colorTexCoord = basetex * vec2(1.0, 0.5) + vec2(0.0, 0.5);\n\n    float depth = depthForPoint(depthTexCoord);\n    float mindepth = depthMin;\n    float maxdepth = depthMax;\n\n    float z = depth * (maxdepth - mindepth) + mindepth;\n    vec2 ortho = basetex * imageDimensions - principalPoint;\n    vec2 proj = ortho * z / focalLength;\n    vec4 worldPos = extrinsics *  vec4(proj.xy, z, 1.0);\n    worldPos.w = 1.0;\n\n    vec4 mvPosition = vec4( worldPos.xyz, 1.0 );\n    mvPosition = modelViewMatrix * mvPosition;\n    ptColor = texture2D(map, colorTexCoord);\n\n    ptColor.a = insideBox3D(worldPos.xyz, thresholdMin, thresholdMax ) > 0.0 && depth > 0.0 ? 1.0 : 0.0;\n\n    mat4 flip = mat4(  vec4(-1.0,0.0,0.0,0.0),\n                        vec4(0.0,1.0,0.0,0.0),\n                        vec4(0.0,0.0,1.0,0.0),\n                        vec4(0.0,0.0,0.0,1.0));\n    \n    gl_Position = projectionMatrix * modelViewMatrix * flip * worldPos;\n    vUv = uv;\n    debug = vec3(1, 0.5, 0.0);\n\n    gl_PointSize = pointSize;\n    gl_PointSize *= ( scale / - mvPosition.z );\n\n}\n"]);
var orthoFrag = glsl(["#define GLSLIFY 1\nuniform sampler2D map;\nuniform float opacity;\nuniform float width;\nuniform float height;\nuniform float depthMin;\nuniform float depthMax;\nuniform vec3 thresholdMin;\nuniform vec3 thresholdMax;\n\nvarying vec4 ptColor;\nvarying vec2 vUv;\nvarying vec3 debug;\n\n#define BRIGHTNESS_THRESHOLD_OFFSET 0.01\n#define FLOAT_EPS 0.00001\n\nconst float _DepthSaturationThreshhold = 0.3; //a given pixel whose saturation is less than half will be culled (old default was .5)\nconst float _DepthBrightnessThreshold = 0.6; //a given pixel whose brightness is less than half will be culled (old default was .9)\n\nvec3 rgb2hsv(vec3 c)\n{\n    vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);\n    vec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g));\n    vec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r));\n    float d = q.x - min(q.w, q.y);\n    return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + FLOAT_EPS)), d / (q.x + FLOAT_EPS), q.x);\n}\n\nfloat depthForPoint(vec2 texturePoint)\n{\n    vec4 depthsample = texture2D(map, texturePoint);\n    vec3 depthsamplehsv = rgb2hsv(depthsample.rgb);\n    return depthsamplehsv.g > _DepthSaturationThreshhold && depthsamplehsv.b > _DepthBrightnessThreshold ? depthsamplehsv.r : 0.0;\n}\n\nvoid main() {\n\n  /*float verticalScale = 480.0 / 720.0;\n  float verticalOffset = 1.0 - verticalScale;\n  vec2 colorUv = vUv * vec2(0.5, verticalScale) + vec2(0, verticalOffset);\n  vec2 depthUv = colorUv + vec2(0.5, 0.0);*/\n\n    float verticalScale = 0.5;//480.0 / 720.0;\n    float verticalOffset = 1.0 - verticalScale;\n\n    vec2 colorUv = vUv * vec2(1.0, verticalScale) + vec2(0.0, 0.5);\n    vec2 depthUv = colorUv - vec2(0.0, 0.5);\n\n    vec4 colorSample = ptColor;// texture2D(map, colorUv); \n    vec4 depthSample = texture2D(map, depthUv); \n\n    vec3 hsv = rgb2hsv(depthSample.rgb);\n    float depth = hsv.b > _DepthBrightnessThreshold + BRIGHTNESS_THRESHOLD_OFFSET ? hsv.r : 0.0;\n    float z = depth * (depthMax - depthMin) + depthMin;\n    float alpha = depth > 0.0 && z > thresholdMin.z && z < thresholdMax.z ? 1.0 : 0.0;\n\n    if(alpha <= 0.0) {\n      discard;\n    }\n\n    colorSample.a *= (alpha * opacity);\n\n    gl_FragColor = colorSample;//vec4(debug, 1);\n}"]);
var orthoVert = glsl(["#define GLSLIFY 1\nuniform sampler2D map;\n\nuniform float pointSize;\nuniform float depthMin;\nuniform float depthMax;\nuniform vec3 thresholdMin;\nuniform vec3 thresholdMax;\nuniform float scale;\nvarying vec4 ptColor;\nvarying vec2 vUv;\nvarying vec3 debug;\n\nconst float _DepthSaturationThreshhold = 0.3; //a given pixel whose saturation is less than half will be culled (old default was .5)\nconst float _DepthBrightnessThreshold = 0.3; //a given pixel whose brightness is less than half will be culled (old default was .9)\nconst float  _Epsilon = .03;\n\n//https://github.com/tobspr/GLSL-Color-Spaces/blob/master/ColorSpaces.inc.glsl\nconst float SRGB_GAMMA = 1.0 / 2.2;\nconst float SRGB_INVERSE_GAMMA = 2.2;\nconst float SRGB_ALPHA = 0.055;\n\n// Converts a single srgb channel to rgb\nfloat srgb_to_linear(float channel) {\n  if (channel <= 0.04045)\n      return channel / 12.92;\n  else\n      return pow((channel + SRGB_ALPHA) / (1.0 + SRGB_ALPHA), 2.4);\n}\n\n// Converts a srgb color to a linear rgb color (exact, not approximated)\nvec3 srgb_to_rgb(vec3 srgb) {\n  return vec3(\n      srgb_to_linear(srgb.r),\n      srgb_to_linear(srgb.g),\n      srgb_to_linear(srgb.b)\n  );\n}\n\n//faster but noisier\nvec3 srgb_to_rgb_approx(vec3 srgb) {\nreturn pow(srgb, vec3(SRGB_INVERSE_GAMMA));\n}\n\nvec3 rgb2hsv(vec3 c)\n{\n  vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);\n  vec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g));\n  vec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r));\n\n  float d = q.x - min(q.w, q.y);\n  return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + _Epsilon)), d / (q.x + _Epsilon), q.x);\n}\n\nfloat depthForPoint(vec2 texturePoint)\n{\n  vec4 depthsample = texture2D(map, texturePoint);\n  vec3 linear = srgb_to_rgb( depthsample.rgb);\n  vec3 depthsamplehsv = rgb2hsv(linear.rgb);\n  return depthsamplehsv.g > _DepthSaturationThreshhold && depthsamplehsv.b > _DepthBrightnessThreshold ? depthsamplehsv.r : 0.0;\n}\n\nvoid main()\n{\n  float mindepth = depthMin;\n  float maxdepth = depthMax;\n\n  float verticalScale = 0.5;//480.0 / 720.0;\n  float verticalOffset = 1.0 - verticalScale;\n\n  vec2 colorUv = uv * vec2(1.0, verticalScale) + vec2(0.0, 0.5);\n  vec2 depthUv = colorUv - vec2(0.0, 0.5);\n\n  float depth = depthForPoint(depthUv);\n\n  float z = depth * (maxdepth - mindepth) + mindepth;\n  \n  vec4 worldPos = vec4(position.xy, -z, 1.0);\n  worldPos.w = 1.0;\n\n  vec4 mvPosition = vec4( worldPos.xyz, 1.0 );\n  mvPosition = modelViewMatrix * mvPosition;\n\n  ptColor = texture2D(map, colorUv);\n\n  gl_Position = projectionMatrix * modelViewMatrix * worldPos;\n  vUv = uv;\n  debug = vec3(1, 0.5, 0.0);\n  \n  gl_PointSize = pointSize;\n  gl_PointSize *= ( scale / - mvPosition.z );\n\n  //gl_Position =  projectionMatrix * modelViewMatrix * vec4(position,1.0);\n}"]);
var cutoutFrag = glsl(["#define GLSLIFY 1\nuniform sampler2D map;\nuniform float opacity;\nuniform float width;\nuniform float height;\nuniform float depthMin;\nuniform float depthMax;\nuniform vec3 thresholdMin;\nuniform vec3 thresholdMax;\n\nvarying vec2 vUv;\n\n#define BRIGHTNESS_THRESHOLD_OFFSET 0.01\n#define FLOAT_EPS 0.00001\n\nconst float _DepthSaturationThreshhold = 0.3; //a given pixel whose saturation is less than half will be culled (old default was .5)\nconst float _DepthBrightnessThreshold = 0.4; //a given pixel whose brightness is less than half will be culled (old default was .9)\n\nvec3 rgb2hsv(vec3 c)\n{\n  vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);\n  vec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g));\n  vec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r));\n  float d = q.x - min(q.w, q.y);\n  return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + FLOAT_EPS)), d / (q.x + FLOAT_EPS), q.x);\n}\n\nvoid main() {\n\n  float verticalScale = 0.5;//480.0 / 720.0;\n  float verticalOffset = 1.0 - verticalScale;\n\n  vec2 colorUv = vUv * vec2(1.0, verticalScale) + vec2(0.0, 0.5);\n  vec2 depthUv = colorUv - vec2(0.0, 0.5);\n\n  vec4 colorSample = texture2D(map, colorUv); \n  vec4 depthSample = texture2D(map, depthUv); \n\n  vec3 hsv = rgb2hsv(depthSample.rgb);\n  float depth = hsv.b > _DepthBrightnessThreshold + BRIGHTNESS_THRESHOLD_OFFSET ? hsv.r : 0.0;\n  float z = depth * (depthMax - depthMin) + depthMin;\n  float alpha = depth > 0.0 && z > thresholdMin.z && z < thresholdMax.z ? 1.0 : 0.0;\n\n  if(alpha <= 0.0) {\n    discard;\n  }\n\n  colorSample.a *= (alpha * opacity);\n\n  gl_FragColor = colorSample;\n}"]);
var cutoutVert = glsl(["#define GLSLIFY 1\nvarying vec2 vUv;\nuniform float pointSize;\nuniform float depthMin;\nuniform float depthMax;\nuniform float scale;\nuniform vec3 thresholdMin;\nuniform vec3 thresholdMax;\n\nvoid main()\n{\n  vUv = uv;\n  gl_Position =  projectionMatrix * modelViewMatrix * vec4(position,1.0);\n}"]);
var HLS_TIMEOUT = 2500;
var schema = {
  videoPath: {
    type: 'string'
  },
  meta: {
    type: 'object',
    defaults: {}
  },
  startat: {
    type: 'number',
    "default": 0
  },
  renderMode: {
    type: 'string',
    "default": 'perspective'
  },
  depthMin: {
    type: 'number',
    "default": 0.29
  },
  depthMax: {
    type: 'number',
    "default": 4.0
  },
  pointSize: {
    type: 'number',
    "default": 8.0
  },
  scale: {
    type: 'number',
    "default": 1.0
  },
  textureSize: {
    type: 'vec2',
    "default": {
      w: 320,
      h: 240
    }
  },
  thresholdMin: {
    type: 'vec3',
    "default": {
      x: -2.0,
      y: -2.0,
      z: 0.0
    }
  },
  thresholdMax: {
    type: 'vec3',
    "default": {
      x: 2.0,
      y: 2.0,
      z: 4.0
    }
  }
};
var STREAMEVENTS = Object.freeze({
  PLAY_SUCCESS: "PLAY_SUCCESS",
  PLAY_ERROR: "PLAY_ERROR",
  LOAD_ERROR: "LOAD_ERROR",
  NETWORK_ERROR: "NETWORK_ERROR",
  MEDIA_ERROR: "MEDIA_ERROR",
  HLS_ERROR: "HLS_ERROR"
});
module.exports = VPTStream; //Volumetric Performance Toolbox streaming player

var VPTStream = /*#__PURE__*/function (_THREE$Object3D) {
  _inherits(VPTStream, _THREE$Object3D);

  var _super = _createSuper(VPTStream);

  function VPTStream() {
    var _this2;

    _classCallCheck(this, VPTStream);

    _this2 = _super.call(this);
    _this2.video = _this2.createVideoEl();
    _this2.texture = new THREE.VideoTexture(_this2.video);
    _this2.texture.minFilter = THREE.NearestFilter;
    _this2.texture.magFilter = THREE.LinearFilter;
    _this2.texture.format = THREE.RGBFormat;
    _this2.hls = null; //When using vptstream in mozilla hubs/spoke we run into issues with the proxy / cors setup and the way Hls resolves urls.
    //Hack copied from her: https://github.com/mozilla/hubs/blob/584e48ad0ccc0da1fc9781e7686d19431a2340cd/src/components/media-views.js#L773
    //the function params / signature is (xhr, u)  

    _this2.hls_xhroverride = null;
    _this2.loadTime = 0;
    _this2.playing = false;
    _this2.meshScalar = 2;
    _this2.params = {};
    return _this2;
  }

  _createClass(VPTStream, [{
    key: "LoadTime",
    get: function get() {
      return this.loadTime;
    }
  }, {
    key: "Playing",
    get: function get() {
      return this.playing;
    }
  }, {
    key: "updateParameter",
    value: function updateParameter(param, value) {
      if (param == "startat") {
        this.video.currentTime = value;
      } else {
        if (this.material) {
          this.material.uniforms[param].value = value;
        }
      }
    }
  }, {
    key: "load",
    value: function load(params) {
      console.log("vptstream load");

      for (var property in schema) {
        console.log("".concat(property, " value:").concat(params[property], " default:").concat(schema[property]["default"]));
        this.params[property] = params.hasOwnProperty(property) ? params[property] : schema[property]["default"];
      }

      if (this.params.meta.hasOwnProperty("depthFocalLength")) {
        this.setProps(this.params.meta);
      } else {
        console.error("invalid meta data for perspective rendering, default to cutout");
        this.params.renderMode = "cutout";
      } //so far we have not had to use custom extrinsice for Azure Kinect or Realsense
      //default could suffice as the alignment is done upstream, when we grab if from the sensor
      //leaving this here to allow for textures that still need alignment 


      var extrinsics = new THREE.Matrix4();
      var ex = this.props.extrinsics;
      extrinsics.set(ex["e00"], ex["e10"], ex["e20"], ex["e30"], ex["e01"], ex["e11"], ex["e21"], ex["e31"], ex["e02"], ex["e12"], ex["e22"], ex["e32"], ex["e03"], ex["e13"], ex["e23"], ex["e33"]);

      if (this.material) {
        console.log("Material exists, dispose");
        this.material.dispose();
        var child = this.getObjectByName("VolumetricVideo");

        if (child) {
          console.log("VolumetricVideo exists, remove");
          this.remove(child);
        }
      }

      this.startVideo(this.params.videoPath);
      var geometry = new THREE.PlaneBufferGeometry(1, 1, this.params.textureSize.w, this.params.textureSize.h);

      switch (this.params.renderMode) {
        case "ortho":
          this.material = new THREE.ShaderMaterial({
            uniforms: {
              "map": {
                type: "t",
                value: this.texture
              },
              "time": {
                type: "f",
                value: 0.0
              },
              "opacity": {
                type: "f",
                value: 1.0
              },
              "pointSize": {
                type: "f",
                value: this.params.pointSize
              },
              "depthMin": {
                type: "f",
                value: this.params.depthMin
              },
              "depthMax": {
                type: "f",
                value: this.params.depthMax
              },
              "thresholdMin": {
                value: this.params.thresholdMin
              },
              "thresholdMax": {
                value: this.params.thresholdMax
              },
              "scale": {
                value: this.params.scale
              },
              extensions: {
                derivatives: true
              }
            },
            side: THREE.DoubleSide,
            vertexShader: orthoVert,
            fragmentShader: orthoFrag,
            transparent: true //depthWrite:falses

          });
          var pointsO = new THREE.Points(geometry, this.material);
          pointsO.position.y = 1;
          pointsO.name = "VolumetricVideo";
          this.add(pointsO);
          break;

        case "cutout":
          this.material = new THREE.ShaderMaterial({
            uniforms: {
              "map": {
                type: "t",
                value: this.texture
              },
              "time": {
                type: "f",
                value: 0.0
              },
              "opacity": {
                type: "f",
                value: 1.0
              },
              "depthMin": {
                type: "f",
                value: this.params.depthMin
              },
              "depthMax": {
                type: "f",
                value: this.params.depthMax
              },
              "thresholdMin": {
                value: this.params.thresholdMin
              },
              "thresholdMax": {
                value: this.params.thresholdMax
              },
              "scale": {
                value: this.params.scale
              },
              extensions: {
                derivatives: true
              }
            },
            side: THREE.DoubleSide,
            vertexShader: cutoutVert,
            fragmentShader: cutoutFrag,
            transparent: true
          });
          var plane = new THREE.Mesh(geometry, this.material);
          plane.position.y = 1;
          plane.scale.set(this.params.textureSize.w / this.params.textureSize.h, 1.0, 1.0);
          plane.name = "VolumetricVideo";
          this.add(plane);
          break;

        case "perspective":
          //assumes depthkit style hsv encoding
          //Material
          this.material = new THREE.ShaderMaterial({
            uniforms: {
              "map": {
                type: "t",
                value: this.texture
              },
              "pointSize": {
                type: "f",
                value: this.params.pointSize
              },
              "depthMin": {
                type: "f",
                value: this.params.depthMin
              },
              "depthMax": {
                type: "f",
                value: this.params.depthMax
              },
              "scale": {
                value: this.params.scale
              },
              "focalLength": {
                value: this.props.depthFocalLength
              },
              "principalPoint": {
                value: this.props.depthPrincipalPoint
              },
              "imageDimensions": {
                value: this.props.depthImageSize
              },
              "width": {
                value: this.props.textureWidth
              },
              "height": {
                value: this.props.textureHeight
              },
              "thresholdMin": {
                value: this.params.thresholdMin
              },
              "thresholdMax": {
                value: this.params.thresholdMax
              },
              "extrinsics": {
                value: extrinsics
              },
              "opacity": {
                type: "f",
                value: 1.0
              }
            },
            extensions: {
              derivatives: true
            },
            vertexShader: rgbdVert,
            fragmentShader: rgbdFrag,
            transparent: true
          }); //Make the shader material double sided

          this.material.side = THREE.DoubleSide;
          var pointP = new THREE.Points(geometry, this.material);
          pointP.name = "VolumetricVideo";
          pointP.position.y = 1;
          this.add(pointP);
          break;

        case "perspective_rl2":
          //assuming librealsense2 hsv colorizer
          //https://dev.intelrealsense.com/docs/depth-image-compression-by-colorization-for-intel-realsense-depth-cameras#section-6references
          //Material
          this.material = new THREE.ShaderMaterial({
            uniforms: {
              "map": {
                type: "t",
                value: this.texture
              },
              "pointSize": {
                type: "f",
                value: this.params.pointSize
              },
              "depthMin": {
                type: "f",
                value: this.params.depthMin
              },
              "depthMax": {
                type: "f",
                value: this.params.depthMax
              },
              "scale": {
                value: this.params.scale
              },
              "focalLength": {
                value: this.props.depthFocalLength
              },
              "principalPoint": {
                value: this.props.depthPrincipalPoint
              },
              "imageDimensions": {
                value: this.props.depthImageSize
              },
              "width": {
                value: this.props.textureWidth
              },
              "height": {
                value: this.props.textureHeight
              },
              "thresholdMin": {
                value: this.params.thresholdMin
              },
              "thresholdMax": {
                value: this.params.thresholdMax
              },
              "extrinsics": {
                value: extrinsics
              },
              "opacity": {
                type: "f",
                value: 1.0
              }
            },
            extensions: {
              derivatives: true
            },
            vertexShader: rgbdVert_rs2,
            fragmentShader: rgbdFrag_rs2,
            transparent: true
          }); //Make the shader material double sided

          this.material.side = THREE.DoubleSide;
          var pointPRL2 = new THREE.Points(geometry, this.material);
          pointPRL2.name = "VolumetricVideo";
          pointPRL2.position.y = 1;
          this.add(pointPRL2);
          break;
      }
    } //load depth camera properties for perspective reprojection

  }, {
    key: "loadPropsFromFile",
    value: function loadPropsFromFile(filePath) {
      var _this3 = this;

      return new Promise(function (resolve, reject) {
        var jsonLoader = new THREE.FileLoader(_this3.manager);
        jsonLoader.setResponseType('json');
        jsonLoader.load(filePath, function (data) {
          resolve(data);
        }, null, function (err) {
          reject(err);
        });
      });
    } //set perspective projection properties

  }, {
    key: "setProps",
    value: function setProps(_props) {
      this.props = _props;

      if (this.props.textureWidth == undefined || this.props.textureHeight == undefined) {
        this.props.textureWidth = this.props.depthImageSize.x;
        this.props.textureHeight = this.props.depthImageSize.y * 2;
      }

      if (this.props.extrinsics == undefined) {
        this.props.extrinsics = {
          e00: 1,
          e01: 0,
          e02: 0,
          e03: 0,
          e10: 0,
          e11: 1,
          e12: 0,
          e13: 0,
          e20: 0,
          e21: 0,
          e22: 1,
          e23: 0,
          e30: 0,
          e31: 0,
          e32: 0,
          e33: 1
        };
      }

      if (this.props.crop == undefined) {
        this.props.crop = {
          x: 0,
          y: 0,
          z: 1,
          w: 1
        };
      }
    }
  }, {
    key: "play",
    value: function play() {
      this.video.play().then(function () {
        this.dispatchEvent({
          type: STREAMEVENTS.PLAY_SUCCESS,
          message: "autoplay success"
        });
        this.playing = true;
      })["catch"](function (error) {
        this.dispatchEvent({
          type: STREAMEVENTS.PLAY_ERROR,
          message: "autoplay error"
        });
        this.playing = false;
      });
      return this.playing;
    }
  }, {
    key: "stop",
    value: function stop() {
      this.video.stop();
    }
  }, {
    key: "pause",
    value: function pause() {}
  }, {
    key: "setVolume",
    value: function setVolume(volume) {
      this.video.volume = volume;
    }
  }, {
    key: "update",
    value: function update(time) {
      this._material.uniforms.time.value = time;
    }
  }, {
    key: "createVideoEl",
    value: function createVideoEl() {
      var el = document.createElement("video");
      el.setAttribute("id", "volumetric-stream-video");
      el.setAttribute("playsinline", "");
      el.setAttribute("webkit-playsinline", ""); // iOS Safari requires the autoplay attribute, or it won't play the video at all.

      el.autoplay = true; // iOS Safari will not play videos without user interaction. We mute the video so that it can autoplay and then
      // allow the user to unmute it with an interaction in the unmute-video-button component.

      el.muted = false;
      el.preload = "auto";
      el.crossOrigin = "anonymous";
      console.log("Volumetric Stream: Video element created", el);
      return el;
    }
  }, {
    key: "scaleToAspectRatio",
    value: function scaleToAspectRatio(el, ratio) {
      var width = Math.min(1.0, 1.0 / ratio);
      var height = Math.min(1.0, ratio);
      el.object3DMap.mesh.scale.set(width, height, 1);
      el.object3DMap.mesh.matrixNeedsUpdate = true;
    }
  }, {
    key: "dispose",
    value: function dispose() {
      if (this.texture.image instanceof HTMLVideoElement) {
        var video = this.texture.image;
        video.pause();
        video.src = "";
        video.load();
      }

      if (this.hls) {
        this.hls.stopLoad();
        this.hls.detachMedia();
        this.hls.destroy();
        this.hls = null;
      }

      this.texture.dispose();
      this.material.dispose();
    }
  }, {
    key: "setVideoUrl",
    value: function setVideoUrl(videoUrl) {
      if (this.hls) {
        this.startVideo(videoUrl);
      }
    }
  }, {
    key: "startVideo",
    value: function startVideo(videoUrl) {
      var _this4 = this;

      console.log("startVideo " + videoUrl);

      if (Hls.isSupported()) {
        var baseUrl = videoUrl;

        var setupHls = function setupHls() {
          if (_this4.hls) {
            _this4.hls.stopLoad();

            _this4.hls.detachMedia();

            _this4.hls.destroy();

            _this4.hls = null;
          } //do we need to hook / override Hls xhr calls to handle cors proxying


          if (_this4.hls_xhroverride) {
            _this4.hls = new Hls({
              xhrSetup: _this4.hls_xhroverride
            });
          } else {
            _this4.hls = new Hls();
          }

          _this4.hls.loadSource(videoUrl);

          _this4.hls.attachMedia(_this4.video);

          _this4.hls.on(Hls.Events.ERROR, function (event, data) {
            if (data.fatal) {
              switch (data.type) {
                case Hls.ErrorTypes.NETWORK_ERROR:
                  //console.log("NETWORK_ERROR", data )
                  _this4.dispatchEvent({
                    type: STREAMEVENTS.NETWORK_ERROR,
                    message: data.message
                  }); // try to recover network error


                  _this4.hls.startLoad();

                  break;

                case Hls.ErrorTypes.MEDIA_ERROR:
                  //console.log("MEDIA_ERROR", data )
                  _this4.dispatchEvent({
                    type: STREAMEVENTS.MEDIA_ERROR,
                    message: data.message
                  });

                  _this4.hls.recoverMediaError();

                  break;

                default:
                  //console.log("Hls ERROR", data )
                  _this4.dispatchEvent({
                    type: STREAMEVENTS.HLS_ERROR,
                    message: "hls error ".concat(data.type, " ").concat(data.message)
                  });

                  return;
              }
            } else {
              console.log("Hls non fatar error:", data);

              if (data.type == Hls.ErrorTypes.MEDIA_ERROR) {//this.hls.recoverMediaError();
              }
            }
          });

          _this4.hls.on(Hls.Events.MANIFEST_PARSED, function (event, data) {
            _this4.loadTime = performance.now();
            var _this = _this4;

            _this4.video.play().then(function () {
              console.log("Hls success auto playing " + _this.params.startat);
              _this.video.currentTime = _this.params.startat;

              _this.dispatchEvent({
                type: STREAMEVENTS.PLAY_SUCCESS,
                message: "autoplay success"
              });

              _this.playing = true;
            })["catch"](function (error) {
              //console.log("Hls error trying to auto play " + error + " " + error.name);
              _this.dispatchEvent({
                type: STREAMEVENTS.PLAY_ERROR,
                message: "error trying to auto play " + error + " " + error.name
              });

              _this.playing = false;
            });
          });
        };

        setupHls();
      } else if (this.video.canPlayType(contentType)) {
        this.video.src = videoUrl;
        this.video.onerror = failLoad;
        this.video.play().then(function () {
          this.dispatchEvent({
            type: STREAMEVENTS.PLAY_SUCCESS,
            message: "autoplay success"
          });
        })["catch"](function (error) {
          this.dispatchEvent({
            type: STREAMEVENTS.PLAY_ERROR,
            message: "autoplay error"
          });
          console.log("error autoplay", data);
        });
      } else {
        console.log("Hls unsupported, can't load or play");
        this.dispatchEvent({
          type: STREAMEVENTS.LOAD_ERROR,
          message: "Hls unsupported, can't play media"
        });
      }
    }
  }], [{
    key: "STREAMEVENTS",
    get: function get() {
      return STREAMEVENTS;
    }
  }]);

  return VPTStream;
}(THREE.Object3D);

exports["default"] = VPTStream;

},{"glslify":1}]},{},[2]);
